{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/topica/anaconda3/envs/workspace/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/topica/anaconda3/envs/workspace/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import cv2\n",
    "# from keras.preprocessing import image\n",
    "# import time\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.preprocessing import image\n",
    "import time\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import align.detect_face\n",
    "\n",
    "\n",
    "\n",
    "# Keras\n",
    "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.models import model_from_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###When you only want to dectect single face, uncomment this\n",
    "\n",
    "# # facenet \n",
    "# image_size=160\n",
    "# margin= 44\n",
    "# gpu_memory_fraction=1.0\n",
    "\n",
    "# def load_and_align_data(img, image_size,margin, gpu_memory_fraction):\n",
    "#     minsize = 20 # minimum size of face\n",
    "#     threshold = [ 0.6, 0.7, 0.7 ]  # three steps's threshold\n",
    "#     factor = 0.709 # scale factor\n",
    "#     with tf.Graph().as_default():\n",
    "#         gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n",
    "#         sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "#         with sess.as_default():\n",
    "#             pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\n",
    "# #     img = scipy.misc.imread(os.path.expanduser(image_path))\n",
    "#     img_size = np.asarray(img.shape)[0:2]\n",
    "#     bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\n",
    "#     if (len(bounding_boxes)==0):\n",
    "#         bb=0\n",
    "#         have_face = 0\n",
    "#     else:\n",
    "#         det = np.squeeze(bounding_boxes[0,0:4])\n",
    "#         bb = np.zeros(4, dtype=np.int32)\n",
    "#         bb[0] = np.maximum(det[0]-margin/2, 0)\n",
    "#         bb[1] = np.maximum(det[1]-margin/2, 0)\n",
    "#         bb[2] = np.minimum(det[2]+margin/2 - bb[0], img_size[1])\n",
    "#         bb[3] = np.minimum(det[3]+margin/2 - bb[1], img_size[0])\n",
    "#         have_face = 1\n",
    "#     return bb,have_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# model = model_from_json(open(\"/home/thaovu/tensorflow-101/model/facial_expression_model_structure.json\", \"r\").read())\n",
    "# model.load_weights('/home/thaovu/tensorflow-101/model/facial_expression_model_weights.h5') #load weights\n",
    "model = model_from_json(open(\"/home/topica/workspace/Facial-Expression-Recognition/model_4layer_2_2_pool.json\", \"r\").read())\n",
    "model.load_weights('/home/topica/workspace/Facial-Expression-Recognition/model_4layer_2_2_pool.h5') #load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###When you only want to dectect single face, uncomment this\n",
    "\n",
    "\n",
    "# cap = cv2.VideoCapture(\"/home/topica/Downloads/2018-06-27_19h.wmv\")\n",
    "# # cap = cv2.VideoCapture(\"/home/topica/Video_topica/T6_08_20180816_090000.avi\")\n",
    "# frame = 0\n",
    "\n",
    "# # fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "# # out = cv2.VideoWriter('output.avi',fourcc, 20.0, (640,480))\n",
    "# # out = cv2.VideoWriter('outpy.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 20, (1024,700))\n",
    "\n",
    "# while(True):\n",
    "#     ret, img = cap.read()\n",
    "#     detect_face, have_face= load_and_align_data(img,image_size,margin,gpu_memory_fraction)\n",
    "#     if (have_face!=0):\n",
    "#         detect_face = np.reshape(detect_face,(-1,4)) \n",
    "#         for (x,y,w,h) in detect_face:\n",
    "#             detected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\n",
    "#             detected_face = cv2.cvtColor(detected_face, cv2.COLOR_BGR2GRAY) #transform to gray scale\n",
    "#             detected_face = cv2.resize(detected_face, (48, 48)) #resize to 48x48\n",
    "#             cv2.rectangle(img,(x,y),(x+w,y+h),(64,64,64),2) #highlight detected face\n",
    "\n",
    "#             img_pixels = image.img_to_array(detected_face)\n",
    "#             img_pixels = np.expand_dims(img_pixels, axis = 0)\n",
    "\n",
    "#             img_pixels /= 255 #pixels are in scale of [0, 255]. normalize all pixels in scale of [0, 1]\n",
    "\n",
    "#             #-----------------------------\n",
    "\n",
    "#             predictions = model.predict(img_pixels) #store probabilities of 7 expressions\n",
    "#             max_index = np.argmax(predictions[0])\n",
    "#             print(max_index)\n",
    "\n",
    "#             #background of expression list\n",
    "#             overlay = img.copy()\n",
    "#             opacity = 0.4\n",
    "#             cv2.rectangle(img,(x+w+15,y-25),(x+w+150,y+115),(64,64,64),cv2.FILLED)\n",
    "#             cv2.addWeighted(overlay, opacity, img, 1 - opacity, 0, img)\n",
    "\n",
    "#             #connect face and expressions\n",
    "#             cv2.line(img,(int((x+x+w)/2),y+15),(x+w,y-20),(255,255,255),1)\n",
    "#             cv2.line(img,(x+w,y-20),(x+w+10,y-20),(255,255,255),1)\n",
    "\n",
    "#             emotion = \"\"\n",
    "#             for i in range(len(predictions[0])):\n",
    "#                 emotion = \"%s %s%s\" % (emotions[i], round(predictions[0][i]*100, 2), '%')\n",
    "\n",
    "#                 \"\"\"if i != max_index:\n",
    "#                     color = (255,0,0)\"\"\"\n",
    "\n",
    "#                 color = (255,0,0)\n",
    "\n",
    "#                 cv2.putText(img, emotion, (int(x+w+15), int(y-12+i*20)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "                \n",
    "#     cv2.imshow('img',img)\n",
    "\n",
    "#     frame = frame + 1\n",
    "#     #print(frame)\n",
    "\n",
    "#     #---------------------------------\n",
    "\n",
    "#     if frame > 227:\n",
    "#         break\n",
    "\n",
    "#     if cv2.waitKey(70) & 0xFF == ord('q'): #press q to quit\n",
    "#         break\n",
    "\n",
    "# #kill open cv things\n",
    "# # out.release()\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ###When you want to dectect multi face\n",
    "image_size=160\n",
    "margin= 44\n",
    "gpu_memory_fraction=1.0\n",
    "detect_multiple_faces = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ###When you want to dectect multi face\n",
    "with tf.Graph().as_default():\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n",
    "    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "    with sess.as_default():\n",
    "        pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\n",
    "frame = 0\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = np.zeros((7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the general feeling of the classroom: happy\n"
     ]
    }
   ],
   "source": [
    "### ###When you want to dectect multi face\n",
    "while(True):\n",
    "    ret, img = cap.read()\n",
    "    minsize = 20 # minimum size of face\n",
    "    threshold = [ 0.6, 0.7, 0.7 ]  # three steps's threshold\n",
    "    factor = 0.709 # scale factor\n",
    "\n",
    "    bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\n",
    "    nrof_faces = bounding_boxes.shape[0]\n",
    "    if nrof_faces>0:\n",
    "        have_face = True\n",
    "        det = bounding_boxes[:,0:4]\n",
    "        det_arr = []\n",
    "        result_face = []\n",
    "        img_size = np.asarray(img.shape)[0:2]\n",
    "        if nrof_faces>1:\n",
    "            if detect_multiple_faces:\n",
    "                for i in range(nrof_faces):\n",
    "                    det_arr.append(np.squeeze(det[i]))\n",
    "            else:\n",
    "                bounding_box_size = (det[:,2]-det[:,0])*(det[:,3]-det[:,1])\n",
    "                img_center = img_size / 2\n",
    "                offsets = np.vstack([ (det[:,0]+det[:,2])/2-img_center[1], (det[:,1]+det[:,3])/2-img_center[0] ])\n",
    "                offset_dist_squared = np.sum(np.power(offsets,2.0),0)\n",
    "                index = np.argmax(bounding_box_size-offset_dist_squared*2.0) # some extra weight on the centering\n",
    "                det_arr.append(det[index,:])\n",
    "        else:\n",
    "            det_arr.append(np.squeeze(det))\n",
    "\n",
    "        for i, det in enumerate(det_arr):\n",
    "            det = np.squeeze(det)\n",
    "            bb = np.zeros(4, dtype=np.int32)\n",
    "            bb[0] = np.maximum(det[0]-margin/2, 0)\n",
    "            bb[1] = np.maximum(det[1]-margin/2, 0)\n",
    "            bb[2] = np.minimum(det[2]+margin/2, img_size[1])\n",
    "            bb[3] = np.minimum(det[3]+margin/2, img_size[0])\n",
    "            cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\n",
    "#             scaled = misc.imresize(cropped, (image_size, image_size), interp='bilinear')\n",
    "            cv2.rectangle(img,(bb[0],bb[1]),(bb[2],bb[3]),(164,64,64),2) #highlight detected face\n",
    "            detected_face = cv2.cvtColor(cropped, cv2.COLOR_BGR2GRAY) #transform to gray scale\n",
    "            detected_face = cv2.resize(detected_face, (48, 48)) #resize to 48x48\n",
    "#             print(detected_face.shape)\n",
    "            img_pixels = image.img_to_array(detected_face)\n",
    "            img_pixels = np.expand_dims(img_pixels, axis = 0)\n",
    "\n",
    "            img_pixels /= 255 #pixels are in scale of [0, 255]. normalize all pixels in scale of [0, 1]\n",
    "\n",
    "            #-----------------------------\n",
    "\n",
    "            predictions = model.predict(img_pixels) #store probabilities of 7 expressions\n",
    "            max_index = np.argmax(predictions[0])\n",
    "            count[max_index] = count[max_index]+1\n",
    "            result = \"%s %s%s\" % (emotions[max_index], round(predictions[0][max_index]*100, 2), '%')\n",
    "            color = (55,55,255)\n",
    "            cv2.putText(img, result, (int(bb[0]+5), int(bb[1]-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1) \n",
    "    cv2.imshow('img',img)\n",
    "    frame = frame + 1\n",
    "    if cv2.waitKey(70) & 0xFF == ord('q'): #press q to quit\n",
    "        break\n",
    "\n",
    "#kill open cv things\n",
    "# out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"the general feeling of the classroom: \" + emotions[np.argsort(count)[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,  75.,  16.,   0., 125.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
